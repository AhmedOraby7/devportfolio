<!DOCTYPE html>
<html class="no-js" lang="en">
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>My Portfolio</title>
    <link rel="shortcut icon" href="favicon.ico" type="image/x-icon" />
    <link rel="icon" href="favicon.ico" type="image/x-icon" />
    <link
      href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="libs/font-awesome/css/font-awesome.min.css" />
    <link href="css/bootstrap.min.css" rel="stylesheet" />
    <link href="css/styles.css" rel="stylesheet" />
  </head>

  <body>
    <div id="mobile-menu-open" class="shadow-large">
      <i class="fa fa-bars" aria-hidden="true"></i>
    </div>
    <!-- End #mobile-menu-toggle -->
    <header>
      <div id="mobile-menu-close">
        <span>Close</span> <i class="fa fa-times" aria-hidden="true"></i>
      </div>
      <ul id="menu" class="shadow">
        <li>
          <a href="#about">About</a>
        </li>
        <li>
          <a href="#experience">Experience</a>
        </li>
        <li>
          <a href="#education">Education</a>
        </li>
        <li>
          <a href="#projects">Projects</a>
        </li>
        <li>
          <a href="#skills">Skills</a>
        </li>
        <li>
          <a href="#contact">Contact</a>
        </li>
      </ul>
    </header>
    <!-- End header -->

    <div id="lead">
      <div id="lead-content">
        <h1>Ahmed ALi</h1>
        <h2>Data Scientist</h2>
        <a href="images/AhmedAliR.pdf" class="btn-rounded-white"
          >Download Resume</a
        >
      </div>
      <!-- End #lead-content -->

      <div id="lead-overlay"></div>

      <div id="lead-down">
        <span>
          <i class="fa fa-chevron-down" aria-hidden="true"></i>
        </span>
      </div>
      <!-- End #lead-down -->
    </div>
    <!-- End #lead -->

    <div id="about">
      <div class="container">
        <div class="row">
          <div class="col-md-4">
            <h2 class="heading">About Me</h2>
          </div>
          <div class="col-md-8">
            <p>
              As a Data Scientist (2 years) and Software Engineer (1 year), I
              bring a fusion of practical experience, academic achievement, and
              a strong passion for data science to the table. With a Bachelor's
              degree in Computer Science and a soon-to-be-completed Master's in
              Data Science from RWTH Aachen, Germany, I've sharpened my skills
              across diverse roles at leading organizations such as Amazon,
              Ejada, and Exporto. Expert in SQL, Python, Tableau, and various
              other tools, I've made impactful contributions to numerous data
              science projects, ranging from the creation of insightful Tableau
              dashboards to the design and implementation of efficient data
              pipelines. My professional journey is complemented by my personal
              commitment to continuously enhance my data science skills through
              personal projects. In my free time, I enjoy maintaining an active
              lifestyle, playing video games, and engaging in personal readings
              primarily focused on data science.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-- End #about -->

    <div id="experience" class="background-alt">
      <h2 class="heading">Experience</h2>
      <div id="experience-timeline">
        <div data-date="March 2023 – Present">
          <h3>DXFACTURE</h3>
          <h4>Data Scientist</h4>
          <p>
            DXFACTURE is a startup company that enables manufacturing companies
            to reduce their energy costs and CO2 emissions - with digital
            solutions and services from a single source! I'm really grateful to
            work with such talented people as a working student
          </p>
          <br />
          <li>
            Developed a comprehensive Tableau dashboard for SweetConnect,
            showcasing CO2 emission and product pricing data, enhancing the
            company’s ability to make environmentally conscious and
            cost-effective decisions.
          </li>
          <li>
            Collaborating with customers on projects to analyze and merge data
            from multiple Excel files.
          </li>
          <li>
            Designing and developing data pipelines (Python) for energy-related
            data, including weather data and prices, to enable efficient and
            reliable data processing and analysis.
          </li>
        </div>

        <div data-date="April 2022 – October 2022">
          <h3>Exporto</h3>
          <h4>Software Engineer</h4>
          <p>
            Exporto is a startup company offering a boardless delivery
            experience for its customers. I worked for Exporto as a working
            student (20 hours/week). My main focus was to improve the runtime
            for some SQL queries as well as improve some of the features in the
            UI. I worked there for almost 6 months and during that period, I had
            the chance to develop/ompitze the following:
          </p>
          <br />
          <li>
            Improved search experience for Exporto’s customers by optimizing SQL
            queries (x2 speed) as well as enhancing the UX.
          </li>
          <li>
            mplemented a global category search from scratch to help Exporto’s
            warehouse worker to process all the parcels efficiently.
          </li>
        </div>

        <div data-date="September 2021 – February 2022">
          <h3>Amazon</h3>
          <h4>Software Engineer Intern</h4>
          <p>
            Working for Amazon was one of the best experiences I've had so far.
            I had the chance to work with a lot of talented people. I worked on
            two different projects. I led both my projects end-to-end which
            means I didn't just develop the back/front end but also wrote unit
            tests and integration tests and finally delivered my projects to
            production. These are the most projects I'm proud of, Why? Because
            these projects have a significant impact on AWS customers as well as
            AWS leadership and they are being used by thousands of people. I
            also helped the team to test their AWS Synthetics locally by running
            them on Chromium instaed of pushing the code to gamma.
          </p>
          <br />
          <li>
            Implemented advanced filters for AWS QuickSetup services from
            scratch.
          </li>
          <li>
            Improved UX experience for QuickSetup so that, every new fresh
            customer knows what the service offers.
          </li>
          <li>
            Implemented a new workflow for AWS Synthetics so that the team can
            run it locally (Previously the team had to deploy to gamma first to
            test their AWS Synthetic).
          </li>
        </div>

        <div data-date="December 2020 – March 2021">
          <h3>RWTH Aachen</h3>
          <h4>Student Assistant</h4>
          <p>
            I worked at RWTH Aachen as a student assistant for one of the
            research projects at the Human Language Technology and Pattern
            Recognition Group (Chair of Computer Science 6). My main task was to
            transcribe the audio files into written text. That was a manual
            process to make the text files ready for a machine translation task.
          </p>
        </div>
        <div data-date="July 2018 – March 2020">
          <h3>Ejada Systems LTD</h3>
          <h4>Data Scientist</h4>
          <p>
            That was my first practical experience in a big company. Ejada is a
            big data company offering its customers big data solutions. I worked
            for Ejada as a data engineer, I was responsible for pulling data
            from multiple resources (SAP Hana, SQL servers, files), process it
            by writing SQL queries and feeding it to our dashboards for business
            analytics.
          </p>
          <br />
          <li>
            Collaborated with the business intelligence team to create a
            comprehensive dashboard using SQL and Tableau, effectively
            summarizing tax payments across various groups and tax types,
            enabling stakeholders to gain valuable insights and make informed
            decisions regarding taxation strategies.
          </li>
          <li>
            Acted as an external resource for the data engineering team,
            successfully delivering 20+ ETL packages to production using
            Microsoft SQL and SSDT, contributing to the efficient management and
            transformation of data for enhanced business analytics and
            decision-making processes.
          </li>
          <li>
            Created a data pipeline to clean and merge tax registration data
            from various Excel files, resulting in a Tableau dashboard that
            provided a timeline of tax submission information for analysis and
            decision-making purposes.
          </li>
        </div>
        <div data-date="April 2018 – July 2018">
          <h3>Software Engineer</h3>
          <h4>Upwork.com</h4>
          <br />
          <p>
            I developed the front-end part for a project that scraps tweets,
            LinkedIn posts, and facebook posts for a user in just one
            application. The main idea was to schedule a cron job that will pull
            the data and feed it to our front-end system.
          </p>
          <br />
          <li>
            Designed and implement React & Redux Application for a completed PHP
            scheduler project which was responsible for posting new
            articles/posts to specific social network applications in the
            scheduled time.
          </li>
          <li>Implemented the full front-end part for the demo project.</li>
        </div>
      </div>
    </div>
    <!-- End #experience -->

    <div id="education">
      <h2 class="heading">Education</h2>
      <div class="education-block">
        <h3>RWTH Aachen</h3>
        <span class="education-date">April 2020 - July 2023</span>
        <h4>Masters of Science</h4>
        <p>
          Data science master's student at RWTH Aachen. My main focus area is
          machine learning and text mining. I already took some courses like
          Automatic Speech Recognition, Machine learning, Advancing machine
          learning, Text mining, and Data visualization and analytics. Also, in
          my seminar, I dived deep into some NLP models like BERT and DeBERTa.
        </p>
        <br />
        <p>
          Master thesis: The objective of my thesis was to enable efficient and
          seamless Ontology-Based-Data-Access (OBDA) within a data lake system
          by integrating a semantic modeling tool. This aimed to empower users
          with the ability to visually create semantic models and export them,
          thereby facilitating data access through SPARQL queries. The
          integration process primarily employed technologies such as Python,
          Flask, and React for the development of the tool, while Apache Spark
          was used to manage the data lake's complex datasets. The successful
          completion of this project demonstrated the capacity to leverage these
          technologies to enhance the accessibility and utility of data within
          large-scale data storage systems.
        </p>
      </div>
      <!-- End .education-block -->

      <div class="education-block">
        <h3>Alexandria University</h3>
        <span class="education-date">Sept 2013 - Sept 2018</span>
        <h4>Bachelor of Science in Computer Science</h4>
        <p>
          My bachelor main focus area was computer science, I had the chance to
          take a lot of general computer science courses as well as speicalized
          ones.
        </p>
        <p>
          Relevant coursework: Analysis and Design of Algorithms, Artificial
          Intelligence, Computer Architecture, Computer Programming, Data
          structures, Database Systems, Discrete Structures for Computing,
          Operating Systems, Optimization Techniques, Pattern Recognition
          Software Engineering.
        </p>
      </div>
      <!-- End .education-block -->
    </div>
    <!-- End #education -->

    <div id="projects" class="background-alt">
      <h2 class="heading">Projects</h2>
      <div class="container">
        <div class="row">
          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/risk.jpeg" width="225" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Real-estate-risk</h3>
              <p>
                In this project, I worked on a subset of data from a nationwide
                title insurance company. The objective was to develop a machine
                learning model to predict the overall title risk for properties.
                By analyzing property information and historical title defect
                data, I employed techniques like Random Forest with PCA and
                SMOTE to address class imbalance. The model achieved a precision
                of 0.879, recall of 0.889, and accuracy of 0.885. Based on the
                results, I made profit-maximizing recommendations for the
                decision threshold that should be used during production.
              </p>
              <a
                href="https://github.com/AhmedOraby7/real-estate-risk/blob/main/real-estate-risk.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/churn.jpeg" width="225" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Supercell churn-prediction</h3>
              <p>
                In this project, user data from Supercell was analyzed to
                predict user churn, a vital metric in the gaming industry. The
                dataset included various user metrics, and through extensive
                exploratory data analysis and feature engineering, meaningful
                features were developed. The application of machine learning
                models, notably Logistic Regression and Random Forest, led to a
                high accuracy of 96% in predicting churn. This project showcases
                the power of data-driven insights in understanding user behavior
                and enhancing user retention. The project code and findings are
                available on GitHub.
              </p>
              <a
                href="https://github.com/AhmedOraby7/churn-prediction/blob/main/churn-pred.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/sales.png" width="225" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Sales prediction</h3>
              <p>
                In this engaging project, historical data from a bookstore was
                utilized to predict the company's survival prospects for the
                upcoming month. Through meticulous exploration of past
                purchasing patterns and customer demands, significant predictors
                were identified. The subsequent application of machine learning
                models resulted in an insightful forecast of the bookstore's
                business trajectory. This project highlights the power of
                predictive analytics in guiding strategic decision-making in the
                retail industry. Details of the project are made available on
                GitHub.
              </p>
              <a
                href="https://github.com/AhmedOraby7/books-sales/blob/main/profit.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/text_summ.png" width="225" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Text summarization with transfomers</h3>
              <p>
                Developed a Custom Transformer Model for Text Summarization:
                Used TensorFlow to engineer a transformer model from the ground
                up, incorporating Dot Product Attention and Causal Attention
                mechanisms, specifically tailored for efficient text
                summarization tasks.
              </p>
              <a
                href="https://github.com/AhmedOraby7/Text-Summarization-TRANSFORMER/blob/main/Text-Summarization-With-Transformer.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/eng-ger.jpeg" width="225" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Machine Translation with Attention</h3>
              <p>
                Developed a translation model between English and German using
                Google Tensorflow, improving the model's capability to handle
                long sentences and complex grammatical structures. The
                architecture consists of an Encoder, Pre-attention Decoder, and
                Main Decoder, where the initial inputs and outputs are processed
                through the encoder and pre-attention decoder, then the
                activations are combined to create the inputs for the attention
                layer. The output of this attention layer is finally passed to
                the main decoder, culminating in the translated output.
              </p>
              <a
                href="https://github.com/AhmedOraby7/Machine-Translation-With-Attention/blob/main/German-English-Translation.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/ETA.png" width="225" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Estimate delivery time</h3>
              <p>
                Conducted exploratory data analysis to identify key trends,
                including peak demand times and differences in delivery times
                across various markets, thereby gaining insights into factors
                influencing delivery times. Implemented machine learning
                algorithms like Linear Regression and Random Forest to predict
                delivery times, optimizing model performance through feature
                selection and hyperparameter tuning. Utilized statistical
                methods such as best subset selection and p-value calculations
                to identify and select the most relevant predictors for the
                model, improving model accuracy and interpretability.
              </p>
              <a
                href="https://github.com/AhmedOraby7/delivery-estimate/blob/main/Estimate_Time.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/Data.jpeg" width="225" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Co2 Emission use case</h3>
              <p>
                Led a Tableau data visualization project for SweetConnect, with
                a focus on CO2 emissions analysis. This initiative significantly
                heightened the company's understanding and awareness of their
                environmental impact.
              </p>
              <a
                href="https://public.tableau.com/app/profile/dx.facture/viz/sweetconnect/InputData"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/Spotify.png" width="275" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Spotify Recommender System</h3>
              <p>
                I built this app using the dataset Spotify published in 2018
                (Milion Playlist Dataset). Of course, due to some computational
                limits, I wasn't able to perform on the whole dataset. I just
                picked 10K playlists to work on.
              </p>
              <p>
                How to use it? First, click on the link below. The project is
                already hosted streamlite and you can easily use it. All you
                have to do is to tell me about your preferences and I will
                recommend you the next song to listen to.
              </p>
              <a
                href="https://ahmedoraby7-spotify-songs-recommender-app-9u00k3.streamlit.app/"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/music.jpeg" width="275" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Music Analysis</h3>
              <p>
                This is a small project to analyze my music streaming history
                over Spotify. I started by first gathering the data from Spotify
                and then using their APIs I scrapped the features I needed for
                each song and started my analysis
              </p>
              <a
                href="https://github.com/AhmedOraby7/music-analysis/blob/main/EDA.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
          <!-- End .project -->

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/IMDB.png" width="275" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Sentiment Analysis</h3>
              <p>
                This is a lab project where I had to predict a review on IMDb
                whether a negative or positive review. The dataset was ready on
                Kaggle. I started with some text mining tasks like removing the
                stop words and standardizing all the tokens and they built and
                test my model.
              </p>
              <a
                href="https://github.com/AhmedOraby7/Imdb-Sentiment-Analysis/blob/master/sentiment_analysis.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>

          <div class="project shadow-large">
            <div class="project-image">
              <img src="images/questions.jpeg" width="275" height="270" />
            </div>
            <!-- End .project-image -->
            <div class="project-info">
              <h3>Question Duplicates?</h3>
              <p>
                That was one of the projects I built in the labs. It's based on
                the Quora dataset. Given two questions A and B, the model will
                decide if this question has been asked before.
              </p>
              <a
                href="https://github.com/AhmedOraby7/Question-duplicates-detector/blob/main/C3_W4_Assignment.ipynb"
                >View Project</a
              >
            </div>
            <!-- End .project-info -->
          </div>
        </div>
      </div>
    </div>
    <!-- End #projects -->

    <div id="education">
      <h2 class="heading">Achievments</h2>
      <div class="education-block">
        <span class="education-date">March 2023</span>
        <p>
          Unsupervised Training of Language Representations
          <a href="images/BERT.pdf">(Link)</a>
        </p>
      </div>
      <div class="education-block">
        <span class="education-date">March 2023</span>
        <p>Data Architect Nanodegree, Udacity</p>
      </div>
      <div class="education-block">
        <span class="education-date">October 2021</span>
        <p>NLP speicaliztion by deeplearning.AI</p>
      </div>
      <div class="education-block">
        <span class="education-date">October 2020</span>
        <p>
          Secured the Third place (Out of 10 Teams) at Facebook hackthon 2020
        </p>
      </div>

      <div class="education-block">
        <span class="education-date">May 2020</span>
        <p>Natural Language Processing Nanodegree, Udacity</p>
      </div>
    </div>
    <!-- End #education -->

    <div id="skills">
      <h2 class="heading">Skills</h2>
      <ul>
        <li>Machine Leanring</li>
        <li>Data Analysis</li>
        <li>Python</li>
        <li>Java</li>
        <li>SQL</li>
        <li>Apache</li>
        <li>Cypress</li>
        <li>React</li>
      </ul>
    </div>
    <!-- End #skills -->

    <div id="contact">
      <h2>Get in Touch</h2>
      <div id="contact-form">
        <form action="https://formspree.io/f/mdovobve" method="POST">
          <label>
            Your email:
            <input type="email" name="email" />
          </label>
          <label>
            Your message:
            <textarea name="message"></textarea>
          </label>
          <!-- your other form fields go here -->
          <button type="submit">Send</button>
        </form>
      </div>
      <!-- End #contact-form -->
    </div>
    <!-- End #contact -->

    <footer>
      <div class="container">
        <div class="row">
          <div class="col-sm-5 copyright">
            <p>
              Copyright &copy; <span id="current-year">2023</span> AHMED ALI
            </p>
          </div>
          <div class="col-sm-2 top">
            <span id="to-top">
              <i class="fa fa-chevron-up" aria-hidden="true"></i>
            </span>
          </div>
          <div class="col-sm-5 social">
            <ul>
              <li>
                <a href="https://github.com/AhmedOraby7" target="_blank"
                  ><i class="fa fa-github" aria-hidden="true"></i
                ></a>
              </li>
              <li>
                <a
                  href="https://www.linkedin.com/in/ahmed-ali-091892146/"
                  target="_blank"
                  ><i class="fa fa-linkedin" aria-hidden="true"></i
                ></a>
              </li>
              <li>
                <a
                  href="https://www.facebook.com/ahmed.oraby.9277583/"
                  target="_blank"
                  ><i class="fa fa-facebook" aria-hidden="true"></i
                ></a>
              </li>
            </ul>
          </div>
        </div>
      </div>
    </footer>
    <!-- End footer -->

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script src="js/scripts.min.js"></script>
  </body>
</html>
